# WebGraph

Artifact release for the paper "WebGraph: Capturing Advertising and Tracking Information Flows for Robust Blocking", published at USENIX Security 2022.
<hr/>

### Requirements

#### Installation

> This project has been run and tested on *Ubuntu 18.04*

First, make sure you have `python3`, `binutils`, `pip`, `gcc `and `g++` installed. Otherwise run the following command

```bash
apt-get install binutils python3-dev python3-pip gcc g++
```

To run all tasks (Graph building, Feature extraction or Classification) on WebGraph, the crawl data used is collected using a custom version of [OpenWPM](https://github.com/sandrasiby/OpenWPM/tree/webgraph). Follow the instructions [here](https://github.com/sandrasiby/OpenWPM/tree/webgraph#readme) to setup OpenWPM in your environment. 

After OpenWPM is installed, if you haven't done it yet, activate the  *conda*  environment: 

```
conda activate openwpm
```

 go into `<project-directory>/code` in the project folder and install the python libraries in `requirements.txt`:

```bash
pip install -r requirements.txt
```

#### Preparing Crawl Data

To generate the crawl data needed for the pipeline, you need to run a crawl using the installed OpenWPM tool. In this repo we provide a [demo-crawl.py ](link/to/demo/file) ##MUST ADD DEMO## to run a sample query. 

```bash
python <project-directory>/demo/demo-crawl.py 
```

After you run the demo, a `datadir` folder will be created in your `demo` directory. Inside the folder, you will find two database files to be used in our pipeline: `crawl-db.sqlite` and `content.ldb`

### Pipeline

with WebGraph, we mainly present 3 tasks that you can run:

1. Graph Preprocessing
2. Classification (training and testing)

#### 1. Graph preprocessing

In this task, WebGraph constructs the dataset for classification by:

- taking your *sqlite* and *leveldb* database files to construct a graph representation of each crawl as explained in the [paper](https://www.usenix.org/system/files/sec22summer_siby.pdf) and export it in a tabular format to a `graph.csv` file and `features.csv` file
- applying the rules from public *filterlists* to label the nodes in each graph and export it in a tabular format to a `labeled.csv` file

To run this task, run the following script:

```
python <project-directory>/code/run.py --input-db <location-to-datadir>/datadir/crawl-db.sqlite --ldb <location-to-datadir>/datadir/content.ldb
```

> All additional arguments accepted by this command:
>
> - `--input-db`: the path to the `.sqlite` file generated by the crawl
> - `--ldb`: the path to the `.ldb` file generated by the crawl 
> - `--features`: the path to the `.yaml` feature categories list. A default `features.yaml` is used if unspecified.
> - `--filters`: the path to the directory to save the filter lists in. A default `filterlists` folder will be created if unspecified.
> - `--out`: the path to the directory of the output `.csv` files.


#### 2. Classification

##TODO##

<hr/>

### Data Schema

#### Graph

These are the columns present in the graph output under `graph.csv`

| Column               | Applies to | Description                                                  |
| -------------------- | ---------- | ------------------------------------------------------------ |
| *visit_id*           | All        | the visit id of the crawl                                    |
| *name*               | All        | the name of the node or edge                                 |
| *graph_attr*         | All        | `Node` or `Edge`                                             |
| *top_level_url*      | All        | -                                                            |
| *attr*               | All        | additional attributes of nodes and edges                     |
| *domain*             | All        | The parent domain of nodes or edges                          |
| *top_level_domain*   | All        | -                                                            |
| *type*               | Node       | The type of node `Document | Element | Request | Script | Storage` |
| *document_url*       | Node       | -                                                            |
| *setter*             | Node       | The name of the node that set storage nodes                  |
| *setting_time_stamp* | Node       | -                                                            |
| *setter_domain*      | Node       | -                                                            |
| *party*              | Node       | the partyness of a node either `first` or `third` or `N/A`   |
| *src*                | Edge       | the source node name of the edge                             |
| *dst*                | Edge       | the destination node name of the edge                        |
| *reqattr*            | Edge       | http request attributes                                      |
| *respattr*           | Edge       | http response attributes                                     |
| *response_status*    | Edge       | http response status                                         |
| *content_hash*       | Edge       | -                                                            |
| *post_body*          | Edge       | -                                                            |
| *post_body_raw*      | Edge       | -                                                            |

#### Features

The features in `features.csv` used are described in [features.yaml](https://github.com/spring-epfl/WebGraph/blob/main/code/features.yaml)

#### Labels

Nodes labeled by either 0 or 1 if they are blocked or not

<hr/>

### Code Organization

### Paper

**WebGraph: Capturing Advertising and Tracking Information Flows for Robust Blocking**
Sandra Siby, Umar Iqbal, Steven Englehardt, Zubair Shafiq, Carmela Troncoso
_USENIX Security Symposium (USENIX), 2022_

**Abstract** -- Users rely on ad and tracker blocking tools to protect their privacy. Unfortunately, existing ad and tracker blocking tools are susceptible to mutable advertising and tracking content.  In this paper, we first demonstrate that a state-of-the-art ad and tracker blocker, AdGraph, is susceptible to such adversarial evasion techniques that are currently deployed on the web. Second, we introduce WebGraph, the first ML-based ad and tracker blocker that detects ads and trackers based on their action rather than their content. By featurizing the actions that  are fundamental to advertising and tracking information flows – e.g., storing an identifier in the browser or sharing an identifier with another tracker – WebGraph  performs nearly as well as prior approaches, but is significantly more robust to adversarial evasions. In particular, we show that WebGraph  achieves comparable accuracy to AdGraph, while significantly decreasing the success rate of an adversary from near-perfect for AdGraph to around 8% for WebGraph. Finally, we show that WebGraph remains robust to sophisticated adversaries that use adversarial evasion techniques beyond those currently deployed on the web.

The full paper can be found [here](https://www.usenix.org/system/files/sec22summer_siby.pdf).



### Citation

If you use the code/data in your research, please cite our work as follows:

```
@inproceedings{Siby22WebGraph,
  title     = {WebGraph: Capturing Advertising and Tracking Information Flows for Robust Blocking},
  author    = {Sandra Siby, Umar Iqbal, Steven Englehardt, Zubair Shafiq, Carmela Troncoso},
  booktitle = {USENIX Security Symposium (USENIX)},
  year      = {2022}
}
```

### Contact

In case of questions, please get in touch with [Sandra Siby](https://sandrasiby.github.io/). 

